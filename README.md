# Instacart Grocery Analytics

![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python)
![SQLite](https://img.shields.io/badge/SQLite-3-003B57?logo=sqlite&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green)

-----

I kept seeing Instacart mentioned in data job descriptions â€” â€œanalyze purchase patternsâ€, â€œimprove reorder predictionsâ€ â€” so I figured Iâ€™d actually dig into their public dataset and see what that work looks like in practice.

The dataset is from a [2017 Kaggle competition](https://www.kaggle.com/competitions/instacart-market-basket-analysis/data): 3.4M orders, ~200K users, 50K products. I focused on SQL analytics â€” the kind of questions an ops or inventory team would actually ask.

-----

## What I was trying to figure out

- Which products do people reliably come back for vs. buy once and forget?
- When are orders actually placed â€” and what does that mean for staffing?
- Do departments have meaningfully different reorder rates?
- Is basket size correlated with anything actionable (like organic produce)?

-----

## How itâ€™s structured

```
instacart-analytics/
â”œâ”€â”€ data/raw/                        # not committed â€” see setup below
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 01_reorder_rates.sql
â”‚   â”œâ”€â”€ 02_demand_by_hour.sql
â”‚   â”œâ”€â”€ 03_top_products.sql
â”‚   â”œâ”€â”€ 04_basket_analysis.sql
â”‚   â””â”€â”€ 05_department_retention.sql
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ instacart_demo.ipynb
â”œâ”€â”€ outputs/                         # charts, committed
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â””â”€â”€ clean.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ run_pipeline.py
```

The pipeline goes: CSVs â†’ SQLite â†’ SQL queries â†’ exported charts. `run_pipeline.py` handles all of it.

-----

## Setup

```bash
git clone https://github.com/yourusername/instacart-analytics.git
cd instacart-analytics
pip install -r requirements.txt
```

Download the data from Kaggle (free account needed):
https://www.kaggle.com/competitions/instacart-market-basket-analysis/data

Unzip into `data/raw/` so you have:

```
orders.csv
order_products__prior.csv
order_products__train.csv
products.csv
departments.csv
aisles.csv
```

Then:

```bash
python run_pipeline.py
jupyter notebook notebooks/instacart_demo.ipynb
```

-----

## What I found

**Reorder rates by department**
Produce sits at ~65% reorder rate. Personal care is around 38%. Makes sense â€” people buy bananas every week, they donâ€™t reorder face wash at the same cadence. The gap is useful for inventory planning: tight stock management for high-reorder categories, more slack for low-reorder ones.

**Order timing**
Peak is 10amâ€“2pm on weekends. Weeknight orders (8pmâ€“midnight) basically drop off. Top hours:

```
10:00 â†’ 284,728 orders
11:00 â†’ 268,916 orders
14:00 â†’ 231,482 orders
```

If youâ€™re thinking about fulfillment capacity, Sunday morning is where it matters.

**Bananas are a weird anchor product**
Theyâ€™re #1 in both volume and reorder rate. My read: people open the app specifically to buy bananas and fill the basket around that. Organic milk, Greek yogurt, avocados follow a similar pattern.

**Organic produce and basket size**
Orders that include organic produce items average ~1.4x the basket size of orders without. Thatâ€™s an interesting promotional angle â€” lead with organic, not discounts.

-----

## One thing that surprised me

The reorder interval data was more consistent than I expected. For top products, the median days-between-orders is pretty tight â€” most people reorder bananas every 7 days, not â€œroughly weekly.â€ That kind of regularity is actually useful for demand forecasting.

Also: `add_to_cart_order` ended up being a decent signal. Products added first tend to have higher reorder rates â€” people build routines around how they shop.

-----

## Why SQLite and not Postgres

The dataset is a static snapshot, single user, no ingestion happening. Standing up a Postgres server for this wouldâ€™ve been overkill. SQLite handles 3.4M rows fine and runs anywhere.

If this were a live pipeline with multiple analysts, Postgres would make sense. This isnâ€™t that.

-----

## What Iâ€™d do differently

The SQL is all flat files right now. Iâ€™d use dbt if I revisited this â€” proper staging/mart layers would make the queries easier to test and the lineage clearer.

Iâ€™d also swap the Matplotlib exports for a Streamlit dashboard. Static charts work but theyâ€™re harder to share.

-----

## Charts

All visualizations are in `/outputs`. The notebook has them inline with context.

-----

ğŸ“§ gaft-2727@outlook.com

 
